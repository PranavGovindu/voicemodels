{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VibeVoice Model - Complete Layer-by-Layer Walkthrough\n",
    "\n",
    "This notebook walks through **every layer** of the VibeVoice model, showing tensor dimensions, annotations, and purpose.\n",
    "\n",
    "**Flow**: Input Embeddings ‚Üí Language Model (28 layers) ‚Üí Tokenizers ‚Üí Connectors ‚Üí Prediction Head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  5.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Setup: Load model\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath('../')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from vibevoice.modular.modeling_vibevoice_inference import VibeVoiceForConditionalGenerationInference\n",
    "\n",
    "model = VibeVoiceForConditionalGenerationInference.from_pretrained(\n",
    "    \"vibevoice/VibeVoice-1.5B\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Language Model - Input Embedding Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(151936, 1536)\n",
      "\n",
      "Weight tensor:\n",
      "  layer.weight.shape = torch.Size([151936, 1536])\n",
      "  layer.weight.dtype = torch.bfloat16\n",
      "\n",
      "Tensor shape breakdown:\n",
      "  shape[0] = 151936 ‚Üí vocab_size (number of tokens in vocabulary)\n",
      "  shape[1] = 1536 ‚Üí hidden_size (embedding dimension)\n",
      "\n",
      "Purpose: Maps token IDs to dense embeddings\n",
      "  Input: token_id (integer) ‚Üí Output: embedding vector [B, 1536]\n",
      "  When used: input_ids [B, L] ‚Üí embeddings [B, L, 1536]\n",
      "\n",
      "This is the FIRST layer - converts discrete tokens to continuous embeddings\n"
     ]
    }
   ],
   "source": [
    "# 1. Language Model Embedding Layer\n",
    "layer = model.model.language_model.embed_tokens\n",
    "print(layer)\n",
    "print(f\"\\nWeight tensor:\")\n",
    "print(f\"  layer.weight.shape = {layer.weight.shape}\")\n",
    "print(f\"  layer.weight.dtype = {layer.weight.dtype}\")\n",
    "\n",
    "print(f\"\\nTensor shape breakdown:\")\n",
    "print(f\"  shape[0] = {layer.weight.shape[0]} ‚Üí vocab_size (number of tokens in vocabulary)\")\n",
    "print(f\"  shape[1] = {layer.weight.shape[1]} ‚Üí hidden_size (embedding dimension)\")\n",
    "\n",
    "print(f\"\\nPurpose: Maps token IDs to dense embeddings\")\n",
    "print(f\"  Input: token_id (integer) ‚Üí Output: embedding vector [B, {layer.weight.shape[1]}]\")\n",
    "print(f\"  When used: input_ids [B, L] ‚Üí embeddings [B, L, {layer.weight.shape[1]}]\")\n",
    "print(f\"\\nThis is the FIRST layer - converts discrete tokens to continuous embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Language Model - Transformer Layers (28 Layers)\n",
    "\n",
    "The model has **28 identical transformer decoder layers**. We'll examine the first layer in detail, then note that this pattern repeats 27 more times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Qwen2DecoderLayer (Layer 0 of 28) ===\n",
      "\n",
      "This layer pattern repeats for all 28 layers\n",
      "\n",
      "--- Self-Attention Projections ---\n",
      "\n",
      "q_proj: Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  q_proj.weight.shape = torch.Size([1536, 1536])\n",
      "  q_proj.weight.dtype = torch.bfloat16\n",
      "  q_proj.bias.shape = torch.Size([1536])\n",
      "  Tensor shape: [out_dim=1536, in_dim=1536]\n",
      "  Purpose: Projects hidden states [B, L, 1536] ‚Üí queries [B, L, 1536]\n",
      "\n",
      "k_proj: Linear(in_features=1536, out_features=256, bias=True)\n",
      "  k_proj.weight.shape = torch.Size([256, 1536])\n",
      "  k_proj.weight.dtype = torch.bfloat16\n",
      "  k_proj.bias.shape = torch.Size([256])\n",
      "  Tensor shape: [key_dim=256, in_dim=1536] (smaller for GQA)\n",
      "  Purpose: Projects hidden states [B, L, 1536] ‚Üí keys [B, L, 256]\n",
      "\n",
      "v_proj: Linear(in_features=1536, out_features=256, bias=True)\n",
      "  v_proj.weight.shape = torch.Size([256, 1536])\n",
      "  v_proj.weight.dtype = torch.bfloat16\n",
      "  v_proj.bias.shape = torch.Size([256])\n",
      "  Tensor shape: [value_dim=256, in_dim=1536] (smaller for GQA)\n",
      "  Purpose: Projects hidden states [B, L, 1536] ‚Üí values [B, L, 256]\n",
      "\n",
      "o_proj: Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  o_proj.weight.shape = torch.Size([1536, 1536])\n",
      "  o_proj.weight.dtype = torch.bfloat16\n",
      "  o_proj.bias = False\n",
      "  Tensor shape: [out_dim=1536, in_dim=1536]\n",
      "  Purpose: Projects attention output [B, L, 1536] ‚Üí [B, L, 1536]\n",
      "\n",
      "‚ö†Ô∏è  This attention block (q_proj, k_proj, v_proj, o_proj) repeats in all 28 layers\n"
     ]
    }
   ],
   "source": [
    "# 2. Language Model Layer 0 - Self-Attention (Q, K, V, O projections)\n",
    "layer = model.model.language_model.layers[0]\n",
    "print(f\"=== Qwen2DecoderLayer (Layer 0 of {len(model.model.language_model.layers)}) ===\")\n",
    "print(f\"\\nThis layer pattern repeats for all {len(model.model.language_model.layers)} layers\")\n",
    "print(\"\\n--- Self-Attention Projections ---\")\n",
    "\n",
    "# Q projection\n",
    "q_proj = layer.self_attn.q_proj\n",
    "print(f\"\\nq_proj: {q_proj}\")\n",
    "print(f\"  q_proj.weight.shape = {q_proj.weight.shape}\")\n",
    "print(f\"  q_proj.weight.dtype = {q_proj.weight.dtype}\")\n",
    "print(f\"  q_proj.bias.shape = {q_proj.bias.shape if q_proj.bias is not None else None}\")\n",
    "print(f\"  Tensor shape: [out_dim={q_proj.weight.shape[0]}, in_dim={q_proj.weight.shape[1]}]\")\n",
    "print(f\"  Purpose: Projects hidden states [B, L, {q_proj.weight.shape[1]}] ‚Üí queries [B, L, {q_proj.weight.shape[0]}]\")\n",
    "\n",
    "# K projection (GQA - smaller dimension)\n",
    "k_proj = layer.self_attn.k_proj\n",
    "print(f\"\\nk_proj: {k_proj}\")\n",
    "print(f\"  k_proj.weight.shape = {k_proj.weight.shape}\")\n",
    "print(f\"  k_proj.weight.dtype = {k_proj.weight.dtype}\")\n",
    "print(f\"  k_proj.bias.shape = {k_proj.bias.shape if k_proj.bias is not None else None}\")\n",
    "print(f\"  Tensor shape: [key_dim={k_proj.weight.shape[0]}, in_dim={k_proj.weight.shape[1]}] (smaller for GQA)\")\n",
    "print(f\"  Purpose: Projects hidden states [B, L, {k_proj.weight.shape[1]}] ‚Üí keys [B, L, {k_proj.weight.shape[0]}]\")\n",
    "\n",
    "# V projection (GQA - smaller dimension)\n",
    "v_proj = layer.self_attn.v_proj\n",
    "print(f\"\\nv_proj: {v_proj}\")\n",
    "print(f\"  v_proj.weight.shape = {v_proj.weight.shape}\")\n",
    "print(f\"  v_proj.weight.dtype = {v_proj.weight.dtype}\")\n",
    "print(f\"  v_proj.bias.shape = {v_proj.bias.shape if v_proj.bias is not None else None}\")\n",
    "print(f\"  Tensor shape: [value_dim={v_proj.weight.shape[0]}, in_dim={v_proj.weight.shape[1]}] (smaller for GQA)\")\n",
    "print(f\"  Purpose: Projects hidden states [B, L, {v_proj.weight.shape[1]}] ‚Üí values [B, L, {v_proj.weight.shape[0]}]\")\n",
    "\n",
    "# O projection\n",
    "o_proj = layer.self_attn.o_proj\n",
    "print(f\"\\no_proj: {o_proj}\")\n",
    "print(f\"  o_proj.weight.shape = {o_proj.weight.shape}\")\n",
    "print(f\"  o_proj.weight.dtype = {o_proj.weight.dtype}\")\n",
    "print(f\"  o_proj.bias = {o_proj.bias is not None}\")\n",
    "print(f\"  Tensor shape: [out_dim={o_proj.weight.shape[0]}, in_dim={o_proj.weight.shape[1]}]\")\n",
    "print(f\"  Purpose: Projects attention output [B, L, {o_proj.weight.shape[1]}] ‚Üí [B, L, {o_proj.weight.shape[0]}]\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  This attention block (q_proj, k_proj, v_proj, o_proj) repeats in all {len(model.model.language_model.layers)} layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MLP (Feed-Forward Network) ---\n",
      "\n",
      "gate_proj: Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  gate_proj.weight.shape = torch.Size([8960, 1536])\n",
      "  gate_proj.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [out=8960, in=1536]\n",
      "  Purpose: Projects [B, L, 1536] ‚Üí gate [B, L, 8960] (GLU gate signal)\n",
      "\n",
      "up_proj: Linear(in_features=1536, out_features=8960, bias=False)\n",
      "  up_proj.weight.shape = torch.Size([8960, 1536])\n",
      "  up_proj.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [out=8960, in=1536]\n",
      "  Purpose: Projects [B, L, 1536] ‚Üí value [B, L, 8960] (GLU value signal)\n",
      "\n",
      "down_proj: Linear(in_features=8960, out_features=1536, bias=False)\n",
      "  down_proj.weight.shape = torch.Size([1536, 8960])\n",
      "  down_proj.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [out=1536, in=8960]\n",
      "  Purpose: Projects GLU output [B, L, 8960] ‚Üí [B, L, 1536]\n",
      "\n",
      "MLP Flow: hidden_size [1536] ‚Üí ffn_dim [8960] ‚Üí hidden_size [1536]\n",
      "‚ö†Ô∏è  This MLP block (gate_proj, up_proj, down_proj) repeats in all 28 layers\n"
     ]
    }
   ],
   "source": [
    "# 2b. Language Model Layer 0 - MLP (Feed-Forward Network)\n",
    "layer = model.model.language_model.layers[0]\n",
    "\n",
    "print(\"--- MLP (Feed-Forward Network) ---\")\n",
    "\n",
    "# Gate projection\n",
    "gate_proj = layer.mlp.gate_proj\n",
    "print(f\"\\ngate_proj: {gate_proj}\")\n",
    "print(f\"  gate_proj.weight.shape = {gate_proj.weight.shape}\")\n",
    "print(f\"  gate_proj.weight.dtype = {gate_proj.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [out={gate_proj.weight.shape[0]}, in={gate_proj.weight.shape[1]}]\")\n",
    "print(f\"  Purpose: Projects [B, L, {gate_proj.weight.shape[1]}] ‚Üí gate [B, L, {gate_proj.weight.shape[0]}] (GLU gate signal)\")\n",
    "\n",
    "# Up projection\n",
    "up_proj = layer.mlp.up_proj\n",
    "print(f\"\\nup_proj: {up_proj}\")\n",
    "print(f\"  up_proj.weight.shape = {up_proj.weight.shape}\")\n",
    "print(f\"  up_proj.weight.dtype = {up_proj.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [out={up_proj.weight.shape[0]}, in={up_proj.weight.shape[1]}]\")\n",
    "print(f\"  Purpose: Projects [B, L, {up_proj.weight.shape[1]}] ‚Üí value [B, L, {up_proj.weight.shape[0]}] (GLU value signal)\")\n",
    "\n",
    "# Down projection\n",
    "down_proj = layer.mlp.down_proj\n",
    "print(f\"\\ndown_proj: {down_proj}\")\n",
    "print(f\"  down_proj.weight.shape = {down_proj.weight.shape}\")\n",
    "print(f\"  down_proj.weight.dtype = {down_proj.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [out={down_proj.weight.shape[0]}, in={down_proj.weight.shape[1]}]\")\n",
    "print(f\"  Purpose: Projects GLU output [B, L, {down_proj.weight.shape[1]}] ‚Üí [B, L, {down_proj.weight.shape[0]}]\")\n",
    "\n",
    "print(f\"\\nMLP Flow: hidden_size [{gate_proj.weight.shape[1]}] ‚Üí ffn_dim [{gate_proj.weight.shape[0]}] ‚Üí hidden_size [{down_proj.weight.shape[0]}]\")\n",
    "print(f\"‚ö†Ô∏è  This MLP block (gate_proj, up_proj, down_proj) repeats in all {len(model.model.language_model.layers)} layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Layer Normalizations ---\n",
      "\n",
      "input_layernorm: Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  input_norm.weight.shape = torch.Size([1536])\n",
      "  input_norm.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [1536] (1D tensor, hidden_size)\n",
      "  Purpose: Normalizes input [B, L, 1536] before self-attention (pre-norm)\n",
      "\n",
      "post_attention_layernorm: Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "  post_norm.weight.shape = torch.Size([1536])\n",
      "  post_norm.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [1536] (1D tensor, hidden_size)\n",
      "  Purpose: Normalizes [B, L, 1536] after attention, before MLP\n",
      "\n",
      "‚ö†Ô∏è  These normalization layers repeat in all 28 transformer layers\n",
      "\n",
      "üìä Summary: Each of the 28 layers contains:\n",
      "   - input_layernorm ‚Üí self_attn (q/k/v/o) ‚Üí post_attention_layernorm ‚Üí MLP (gate/up/down)\n"
     ]
    }
   ],
   "source": [
    "# 2c. Language Model Layer 0 - Layer Normalizations\n",
    "layer = model.model.language_model.layers[0]\n",
    "\n",
    "print(\"--- Layer Normalizations ---\")\n",
    "\n",
    "# Input layer norm\n",
    "input_norm = layer.input_layernorm\n",
    "print(f\"\\ninput_layernorm: {input_norm}\")\n",
    "print(f\"  input_norm.weight.shape = {input_norm.weight.shape}\")\n",
    "print(f\"  input_norm.weight.dtype = {input_norm.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [{input_norm.weight.shape[0]}] (1D tensor, hidden_size)\")\n",
    "print(f\"  Purpose: Normalizes input [B, L, {input_norm.weight.shape[0]}] before self-attention (pre-norm)\")\n",
    "\n",
    "# Post-attention layer norm\n",
    "post_norm = layer.post_attention_layernorm\n",
    "print(f\"\\npost_attention_layernorm: {post_norm}\")\n",
    "print(f\"  post_norm.weight.shape = {post_norm.weight.shape}\")\n",
    "print(f\"  post_norm.weight.dtype = {post_norm.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [{post_norm.weight.shape[0]}] (1D tensor, hidden_size)\")\n",
    "print(f\"  Purpose: Normalizes [B, L, {post_norm.weight.shape[0]}] after attention, before MLP\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  These normalization layers repeat in all {len(model.model.language_model.layers)} transformer layers\")\n",
    "print(f\"\\nüìä Summary: Each of the {len(model.model.language_model.layers)} layers contains:\")\n",
    "print(f\"   - input_layernorm ‚Üí self_attn (q/k/v/o) ‚Üí post_attention_layernorm ‚Üí MLP (gate/up/down)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "\n",
      "  norm.weight.shape = torch.Size([1536])\n",
      "  norm.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [1536] (1D tensor)\n",
      "\n",
      "Purpose: Final normalization layer after all 28 transformer layers\n",
      "  Input: [B, L, 1536] ‚Üí Output: [B, L, 1536] (normalized)\n",
      "\n",
      "‚úÖ After this, we have processed embeddings ready for connectors or prediction head\n"
     ]
    }
   ],
   "source": [
    "# 3. Language Model Final Norm\n",
    "layer = model.model.language_model.norm\n",
    "print(layer)\n",
    "print(f\"\\n  norm.weight.shape = {layer.weight.shape}\")\n",
    "print(f\"  norm.weight.dtype = {layer.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [{layer.weight.shape[0]}] (1D tensor)\")\n",
    "print(f\"\\nPurpose: Final normalization layer after all {len(model.model.language_model.layers)} transformer layers\")\n",
    "print(f\"  Input: [B, L, {layer.weight.shape[0]}] ‚Üí Output: [B, L, {layer.weight.shape[0]}] (normalized)\")\n",
    "print(f\"\\n‚úÖ After this, we have processed embeddings ready for connectors or prediction head\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Acoustic Tokenizer - Encoder (Audio ‚Üí 64D Latents)\n",
    "\n",
    "The encoder has **7 downsample stages** with convolutional layers. We'll examine each stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TokenizerEncoder ===\n",
      "Number of downsample stages: 7\n",
      "Number of processing stages: 7\n",
      "\n",
      "Purpose: Encodes audio waveform ‚Üí 64D acoustic latents\n",
      "Flow: Audio [1 channel] ‚Üí 7 downsample stages ‚Üí Processing blocks ‚Üí 64D output\n",
      "\n",
      "We'll examine each downsample layer and stage block below\n"
     ]
    }
   ],
   "source": [
    "# 4. Acoustic Tokenizer Encoder - Overview\n",
    "encoder = model.model.acoustic_tokenizer.encoder\n",
    "print(\"=== TokenizerEncoder ===\")\n",
    "print(f\"Number of do  wnsample stages: {len(encoder.downsample_layers)}\")\n",
    "print(f\"Number of processing stages: {len(encoder.stages)}\")\n",
    "print(f\"\\nPurpose: Encodes audio waveform ‚Üí 64D acoustic latents\")\n",
    "print(f\"Flow: Audio [1 channel] ‚Üí 7 downsample stages ‚Üí Processing blocks ‚Üí 64D output\")\n",
    "print(f\"\\nWe'll examine each downsample layer and stage block below\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All 7 Downsample Layers ===\n",
      "\n",
      "These layers progressively downsample and increase channels:\n",
      "\n",
      "Stage | Weight Shape | In‚ÜíOut Channels | Kernel | Stride | Output Shape\n",
      "------|--------------|-----------------|--------|--------|-------------\n",
      "  0    | torch.Size([32, 1, 7]) |  1‚Üí32          |      7 |      1 | [B, 32, T/1]\n",
      "  1    | torch.Size([64, 32, 4]) | 32‚Üí64          |      4 |      2 | [B, 64, T/2]\n",
      "  2    | torch.Size([128, 64, 4]) | 64‚Üí128          |      4 |      2 | [B, 128, T/2]\n",
      "  3    | torch.Size([256, 128, 8]) | 128‚Üí256          |      8 |      4 | [B, 256, T/4]\n",
      "  4    | torch.Size([512, 256, 10]) | 256‚Üí512          |     10 |      5 | [B, 512, T/5]\n",
      "  5    | torch.Size([1024, 512, 10]) | 512‚Üí1024          |     10 |      5 | [B, 1024, T/5]\n",
      "  6    | torch.Size([2048, 1024, 16]) | 1024‚Üí2048          |     16 |      8 | [B, 2048, T/8]\n",
      "\n",
      "‚ö†Ô∏è  These 7 downsample layers progressively reduce temporal resolution\n",
      "   and increase channel depth from 1 ‚Üí 32 ‚Üí 64 ‚Üí 128 ‚Üí 256 ‚Üí 512 ‚Üí 1024 ‚Üí 2048\n",
      "   Where B=batch, T=time samples\n"
     ]
    }
   ],
   "source": [
    "# 4a. Acoustic Tokenizer Encoder - All Downsample Layers\n",
    "print(f\"=== All {len(encoder.downsample_layers)} Downsample Layers ===\")\n",
    "print(f\"\\nThese layers progressively downsample and increase channels:\")\n",
    "print(f\"\\nStage | Weight Shape | In‚ÜíOut Channels | Kernel | Stride | Output Shape\")\n",
    "print(f\"------|--------------|-----------------|--------|--------|-------------\")\n",
    "\n",
    "for i, downsample_layer in enumerate(encoder.downsample_layers):\n",
    "    conv = downsample_layer[0].conv.conv\n",
    "    out_ch = conv.weight.shape[0]\n",
    "    in_ch = conv.weight.shape[1]\n",
    "    kernel = conv.weight.shape[2]\n",
    "    stride = conv.stride[0] if hasattr(conv, 'stride') else 1\n",
    "    \n",
    "    print(f\"  {i}    | {conv.weight.shape} | {in_ch:2}‚Üí{out_ch:2}          | {kernel:6} | {stride:6} | [B, {out_ch}, T/{stride}]\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  These {len(encoder.downsample_layers)} downsample layers progressively reduce temporal resolution\")\n",
    "print(f\"   and increase channel depth from 1 ‚Üí 32 ‚Üí 64 ‚Üí 128 ‚Üí 256 ‚Üí 512 ‚Üí 1024 ‚Üí 2048\")\n",
    "print(f\"   Where B=batch, T=time samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Downsample Stage 0 (First of 7) ===\n",
      "Layer: SConv1d(\n",
      "  (conv): NormConv1d(\n",
      "    (conv): Conv1d(1, 32, kernel_size=(7,), stride=(1,))\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv1d:\n",
      "  conv.weight.shape = torch.Size([32, 1, 7])\n",
      "  conv.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [out_ch=32, in_ch=1, kernel=7]\n",
      "  Purpose: Converts [B, 1, T] ‚Üí [B, 32, T']\n",
      "  Where B=batch, T=time samples, T'=downsampled time\n",
      "\n",
      "‚ö†Ô∏è  Similar structure for all 7 downsample layers (different channels/kernels)\n"
     ]
    }
   ],
   "source": [
    "# 4b. Acoustic Tokenizer Encoder - Downsample Layer 0 (Detailed)\n",
    "downsample_0 = encoder.downsample_layers[0][0]\n",
    "conv_0 = downsample_0.conv.conv\n",
    "print(f\"=== Downsample Stage 0 (First of {len(encoder.downsample_layers)}) ===\")\n",
    "print(f\"Layer: {downsample_0}\")\n",
    "print(f\"\\nConv1d:\")\n",
    "print(f\"  conv.weight.shape = {conv_0.weight.shape}\")\n",
    "print(f\"  conv.weight.dtype = {conv_0.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [out_ch={conv_0.weight.shape[0]}, in_ch={conv_0.weight.shape[1]}, kernel={conv_0.weight.shape[2]}]\")\n",
    "print(f\"  Purpose: Converts [B, {conv_0.weight.shape[1]}, T] ‚Üí [B, {conv_0.weight.shape[0]}, T']\")\n",
    "print(f\"  Where B=batch, T=time samples, T'=downsampled time\")\n",
    "print(f\"\\n‚ö†Ô∏è  Similar structure for all {len(encoder.downsample_layers)} downsample layers (different channels/kernels)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All 7 Processing Stages ===\n",
      "\n",
      "Each stage contains Block1D layers that process the downsampled features:\n",
      "\n",
      "Stage | Block1D Count | Channel Dim | Purpose\n",
      "------|---------------|-------------|---------\n",
      "  0    |             3 |          32 | Feature processing at this resolution\n",
      "  1    |             3 |          64 | Feature processing at this resolution\n",
      "  2    |             3 |         128 | Feature processing at this resolution\n",
      "  3    |             3 |         256 | Feature processing at this resolution\n",
      "  4    |             3 |         512 | Feature processing at this resolution\n",
      "  5    |             3 |        1024 | Feature processing at this resolution\n",
      "  6    |             8 |        2048 | Feature processing at this resolution\n",
      "\n",
      "‚ö†Ô∏è  Each stage has multiple Block1D layers for feature extraction\n",
      "   Total Block1D layers across all stages: 26\n"
     ]
    }
   ],
   "source": [
    "# 4c. Acoustic Tokenizer Encoder - Processing Stages Overview\n",
    "print(f\"=== All {len(encoder.stages)} Processing Stages ===\")\n",
    "print(f\"\\nEach stage contains Block1D layers that process the downsampled features:\")\n",
    "print(f\"\\nStage | Block1D Count | Channel Dim | Purpose\")\n",
    "print(f\"------|---------------|-------------|---------\")\n",
    "\n",
    "for i, stage in enumerate(encoder.stages):\n",
    "    num_blocks = len(stage)\n",
    "    # Get channel dim from first block's norm\n",
    "    if len(stage) > 0:\n",
    "        channel_dim = stage[0].norm.weight.shape[0]\n",
    "        print(f\"  {i}    | {num_blocks:13} | {channel_dim:11} | Feature processing at this resolution\")\n",
    "    \n",
    "print(f\"\\n‚ö†Ô∏è  Each stage has multiple Block1D layers for feature extraction\")\n",
    "print(f\"   Total Block1D layers across all stages: {sum(len(stage) for stage in encoder.stages)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Block1D Structure (Stage 0, Block 0 of 3) ===\n",
      "\n",
      "Each Block1D contains:\n",
      "  - ConvRMSNorm (normalization)\n",
      "  - Convlayer (depthwise conv)\n",
      "  - FFN (feed-forward network)\n",
      "\n",
      "‚ö†Ô∏è  This Block1D structure repeats in all stages\n",
      "\n",
      "--- norm (ConvRMSNorm) ---\n",
      "  norm.weight.shape = torch.Size([32])\n",
      "  norm.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [32] (1D tensor, channel_dim)\n",
      "  Purpose: Normalizes [B, 32, T] before convolution\n",
      "\n",
      "--- mixer.conv (Depthwise Conv1d) ---\n",
      "  conv.weight.shape = torch.Size([32, 1, 7])\n",
      "  conv.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [out_ch=32, in_ch=1, kernel=7]\n",
      "  Groups: 32 (depthwise - processes each channel separately)\n",
      "  Purpose: Spatial feature extraction [B, 1, T] ‚Üí [B, 32, T]\n",
      "\n",
      "--- ffn (Feed-Forward Network) ---\n",
      "  ffn.linear1.weight.shape = torch.Size([128, 32]), dtype: torch.bfloat16\n",
      "  ffn.linear2.weight.shape = torch.Size([32, 128]), dtype: torch.bfloat16\n",
      "  Tensor shapes: [32] ‚Üí [128] ‚Üí [32]\n",
      "  Purpose: Channel-wise transformation [B, T, 32] ‚Üí [B, T, 32]\n"
     ]
    }
   ],
   "source": [
    "# 4d. Acoustic Tokenizer Encoder - Block1D Structure (Stage 0, Block 0)\n",
    "stage_0 = encoder.stages[0]\n",
    "block_0 = stage_0[0]\n",
    "print(f\"=== Block1D Structure (Stage 0, Block 0 of {len(stage_0)}) ===\")\n",
    "print(f\"\\nEach Block1D contains:\")\n",
    "print(f\"  - ConvRMSNorm (normalization)\")\n",
    "print(f\"  - Convlayer (depthwise conv)\")\n",
    "print(f\"  - FFN (feed-forward network)\")\n",
    "print(f\"\\n‚ö†Ô∏è  This Block1D structure repeats in all stages\")\n",
    "\n",
    "# Norm\n",
    "norm = block_0.norm\n",
    "print(f\"\\n--- norm (ConvRMSNorm) ---\")\n",
    "print(f\"  norm.weight.shape = {norm.weight.shape}\")\n",
    "print(f\"  norm.weight.dtype = {norm.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [{norm.weight.shape[0]}] (1D tensor, channel_dim)\")\n",
    "print(f\"  Purpose: Normalizes [B, {norm.weight.shape[0]}, T] before convolution\")\n",
    "\n",
    "# Conv in mixer (NormConv1d wraps Conv1d in .conv)\n",
    "conv_mixer = block_0.mixer.conv.conv\n",
    "conv_weight = conv_mixer.conv.weight  # Access the actual Conv1d weight\n",
    "print(f\"\\n--- mixer.conv (Depthwise Conv1d) ---\")\n",
    "print(f\"  conv.weight.shape = {conv_weight.shape}\")\n",
    "print(f\"  conv.weight.dtype = {conv_weight.dtype}\")\n",
    "print(f\"  Tensor shape: [out_ch={conv_weight.shape[0]}, in_ch={conv_weight.shape[1]}, kernel={conv_weight.shape[2]}]\")\n",
    "print(f\"  Groups: {conv_mixer.conv.groups} (depthwise - processes each channel separately)\")\n",
    "print(f\"  Purpose: Spatial feature extraction [B, {conv_weight.shape[1]}, T] ‚Üí [B, {conv_weight.shape[0]}, T]\")\n",
    "\n",
    "# FFN\n",
    "ffn = block_0.ffn\n",
    "print(f\"\\n--- ffn (Feed-Forward Network) ---\")\n",
    "print(f\"  ffn.linear1.weight.shape = {ffn.linear1.weight.shape}, dtype: {ffn.linear1.weight.dtype}\")\n",
    "print(f\"  ffn.linear2.weight.shape = {ffn.linear2.weight.shape}, dtype: {ffn.linear2.weight.dtype}\")\n",
    "print(f\"  Tensor shapes: [{ffn.linear1.weight.shape[1]}] ‚Üí [{ffn.linear1.weight.shape[0]}] ‚Üí [{ffn.linear2.weight.shape[0]}]\")\n",
    "print(f\"  Purpose: Channel-wise transformation [B, T, {ffn.linear1.weight.shape[1]}] ‚Üí [B, T, {ffn.linear2.weight.shape[0]}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Encoder Head Layer (Final Layer) ===\n",
      "Layer: SConv1d(\n",
      "  (conv): NormConv1d(\n",
      "    (conv): Conv1d(2048, 64, kernel_size=(7,), stride=(1,))\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv1d:\n",
      "  head.weight.shape = torch.Size([64, 2048, 7])\n",
      "  head.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [out_ch=64, in_ch=2048, kernel=7]\n",
      "  Purpose: Final projection to 64D acoustic latent space\n",
      "  Input: [B, 2048, T] ‚Üí Output: [B, 64, T]\n",
      "  Note: '64D' means shape [B, 64, T] where B=batch, T=time steps\n",
      "\n",
      "‚úÖ This is the final layer of the encoder - outputs 64D acoustic latents\n"
     ]
    }
   ],
   "source": [
    "# 4e. Acoustic Tokenizer Encoder - Head Layer (Final projection to 64D)\n",
    "head = encoder.head.conv.conv\n",
    "print(f\"=== Encoder Head Layer (Final Layer) ===\")\n",
    "print(f\"Layer: {encoder.head}\")\n",
    "print(f\"\\nConv1d:\")\n",
    "print(f\"  head.weight.shape = {head.weight.shape}\")\n",
    "print(f\"  head.weight.dtype = {head.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [out_ch={head.weight.shape[0]}, in_ch={head.weight.shape[1]}, kernel={head.weight.shape[2]}]\")\n",
    "print(f\"  Purpose: Final projection to 64D acoustic latent space\")\n",
    "print(f\"  Input: [B, {head.weight.shape[1]}, T] ‚Üí Output: [B, {head.weight.shape[0]}, T]\")\n",
    "print(f\"  Note: '64D' means shape [B, 64, T] where B=batch, T=time steps\")\n",
    "print(f\"\\n‚úÖ This is the final layer of the encoder - outputs 64D acoustic latents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Acoustic Tokenizer - Decoder (64D Latents ‚Üí Audio)\n",
    "\n",
    "The decoder has **7 upsample stages** that reconstruct audio from latents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TokenizerDecoder ===\n",
      "Number of upsample stages: 7\n",
      "Number of processing stages: 7\n",
      "\n",
      "Purpose: Decodes 64D acoustic latents ‚Üí audio waveform\n",
      "Flow: 64D latents ‚Üí Upsample stages ‚Üí Processing blocks ‚Üí 1 channel audio\n",
      "\n",
      "This is the reverse of the encoder - reconstructs audio from compressed representation\n"
     ]
    }
   ],
   "source": [
    "# 5. Acoustic Tokenizer Decoder - Overview\n",
    "decoder = model.model.acoustic_tokenizer.decoder\n",
    "print(\"=== TokenizerDecoder ===\")\n",
    "print(f\"Number of upsample stages: {len(decoder.upsample_layers)}\")\n",
    "print(f\"Number of processing stages: {len(decoder.stages)}\")\n",
    "print(f\"\\nPurpose: Decodes 64D acoustic latents ‚Üí audio waveform\")\n",
    "print(f\"Flow: 64D latents ‚Üí Upsample stages ‚Üí Processing blocks ‚Üí 1 channel audio\")\n",
    "print(f\"\\nThis is the reverse of the encoder - reconstructs audio from compressed representation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All 7 Upsample Layers ===\n",
      "\n",
      "These layers progressively upsample and decrease channels:\n",
      "\n",
      "Stage | Out Channels | Type | Purpose\n",
      "------|--------------|------|---------\n",
      "  0    |         2048 | Conv1d | 64‚Üí2048 channels\n",
      "  1    |         2048 | ConvTranspose1d | 1024‚Üí2048, stride=8 (upsample)\n",
      "  2    |         1024 | ConvTranspose1d | 512‚Üí1024, stride=5 (upsample)\n",
      "  3    |          512 | ConvTranspose1d | 256‚Üí512, stride=5 (upsample)\n",
      "  4    |          256 | ConvTranspose1d | 128‚Üí256, stride=4 (upsample)\n",
      "  5    |          128 | ConvTranspose1d | 64‚Üí128, stride=2 (upsample)\n",
      "  6    |           64 | ConvTranspose1d | 32‚Üí64, stride=2 (upsample)\n",
      "\n",
      "‚ö†Ô∏è  These 7 upsample layers progressively increase temporal resolution\n",
      "   and decrease channel depth from 2048 ‚Üí 1024 ‚Üí 512 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 32 ‚Üí 1\n"
     ]
    }
   ],
   "source": [
    "# 5a. Acoustic Tokenizer Decoder - All Upsample Layers\n",
    "print(f\"=== All {len(decoder.upsample_layers)} Upsample Layers ===\")\n",
    "print(f\"\\nThese layers progressively upsample and decrease channels:\")\n",
    "print(f\"\\nStage | Out Channels | Type | Purpose\")\n",
    "print(f\"------|--------------|------|---------\")\n",
    "\n",
    "for i, upsample_layer in enumerate(decoder.upsample_layers):\n",
    "    if len(upsample_layer) > 0:\n",
    "        layer = upsample_layer[0]\n",
    "        if hasattr(layer, 'conv'):\n",
    "            # Regular conv\n",
    "            conv = layer.conv.conv\n",
    "            out_ch = conv.weight.shape[0]\n",
    "            in_ch = conv.weight.shape[1]\n",
    "            print(f\"  {i}    | {out_ch:12} | Conv1d | {in_ch}‚Üí{out_ch} channels\")\n",
    "        elif hasattr(layer, 'convtr'):\n",
    "            # Transposed conv (upsampling)\n",
    "            convtr = layer.convtr.convtr\n",
    "            out_ch = convtr.weight.shape[0]\n",
    "            in_ch = convtr.weight.shape[1]\n",
    "            stride = convtr.stride[0] if hasattr(convtr, 'stride') else 1\n",
    "            print(f\"  {i}    | {out_ch:12} | ConvTranspose1d | {in_ch}‚Üí{out_ch}, stride={stride} (upsample)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  These {len(decoder.upsample_layers)} upsample layers progressively increase temporal resolution\")\n",
    "print(f\"   and decrease channel depth from 2048 ‚Üí 1024 ‚Üí 512 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 32 ‚Üí 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Decoder Head Layer (Final Layer) ===\n",
      "Layer: SConv1d(\n",
      "  (conv): NormConv1d(\n",
      "    (conv): Conv1d(32, 1, kernel_size=(7,), stride=(1,))\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv1d:\n",
      "  head.weight.shape = torch.Size([1, 32, 7])\n",
      "  head.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [out_ch=1, in_ch=32, kernel=7]\n",
      "  Purpose: Final projection to 1-channel audio waveform\n",
      "  Input: [B, 32, T] ‚Üí Output: [B, 1, T]\n",
      "  Where B=batch, T=time samples (audio waveform)\n",
      "\n",
      "‚úÖ This is the final layer of the decoder - outputs reconstructed audio\n"
     ]
    }
   ],
   "source": [
    "# 5b. Acoustic Tokenizer Decoder - Head Layer (Final projection to 1 channel)\n",
    "head = decoder.head.conv.conv\n",
    "print(f\"=== Decoder Head Layer (Final Layer) ===\")\n",
    "print(f\"Layer: {decoder.head}\")\n",
    "print(f\"\\nConv1d:\")\n",
    "print(f\"  head.weight.shape = {head.weight.shape}\")\n",
    "print(f\"  head.weight.dtype = {head.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [out_ch={head.weight.shape[0]}, in_ch={head.weight.shape[1]}, kernel={head.weight.shape[2]}]\")\n",
    "print(f\"  Purpose: Final projection to 1-channel audio waveform\")\n",
    "print(f\"  Input: [B, {head.weight.shape[1]}, T] ‚Üí Output: [B, {head.weight.shape[0]}, T]\")\n",
    "print(f\"  Where B=batch, T=time samples (audio waveform)\")\n",
    "print(f\"\\n‚úÖ This is the final layer of the decoder - outputs reconstructed audio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Semantic TokenizerEncoder ===\n",
      "Number of downsample stages: 7\n",
      "Number of processing stages: 7\n",
      "\n",
      "Purpose: Encodes audio waveform ‚Üí 128D semantic latents\n",
      "Key difference: Outputs 128D (semantic) vs 64D (acoustic)\n",
      "Captures: Linguistic content (what is said) not acoustic properties (how it sounds)\n"
     ]
    }
   ],
   "source": [
    "# 6. Semantic Tokenizer Encoder - Overview\n",
    "semantic_encoder = model.model.semantic_tokenizer.encoder\n",
    "print(\"=== Semantic TokenizerEncoder ===\")\n",
    "print(f\"Number of downsample stages: {len(semantic_encoder.downsample_layers)}\")\n",
    "print(f\"Number of processing stages: {len(semantic_encoder.stages)}\")\n",
    "print(f\"\\nPurpose: Encodes audio waveform ‚Üí 128D semantic latents\")\n",
    "print(f\"Key difference: Outputs 128D (semantic) vs 64D (acoustic)\")\n",
    "print(f\"Captures: Linguistic content (what is said) not acoustic properties (how it sounds)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All 7 Downsample Layers ===\n",
      "\n",
      "Stage | Out Channels | In‚ÜíOut | Purpose\n",
      "------|--------------|--------|---------\n",
      "  0    |           32 |  1‚Üí32  | Downsample + channel expansion\n",
      "  1    |           64 | 32‚Üí64  | Downsample + channel expansion\n",
      "  2    |          128 | 64‚Üí128  | Downsample + channel expansion\n",
      "  3    |          256 | 128‚Üí256  | Downsample + channel expansion\n",
      "  4    |          512 | 256‚Üí512  | Downsample + channel expansion\n",
      "  5    |         1024 | 512‚Üí1024  | Downsample + channel expansion\n",
      "  6    |         2048 | 1024‚Üí2048  | Downsample + channel expansion\n",
      "\n",
      "‚ö†Ô∏è  Same structure as acoustic encoder: 7 downsample stages\n",
      "   Channel progression: 1 ‚Üí 32 ‚Üí 64 ‚Üí 128 ‚Üí 256 ‚Üí 512 ‚Üí 1024 ‚Üí 2048\n"
     ]
    }
   ],
   "source": [
    "# 6a. Semantic Tokenizer Encoder - All Downsample Layers\n",
    "print(f\"=== All {len(semantic_encoder.downsample_layers)} Downsample Layers ===\")\n",
    "print(f\"\\nStage | Out Channels | In‚ÜíOut | Purpose\")\n",
    "print(f\"------|--------------|--------|---------\")\n",
    "\n",
    "for i, downsample_layer in enumerate(semantic_encoder.downsample_layers):\n",
    "    conv = downsample_layer[0].conv.conv\n",
    "    out_ch = conv.weight.shape[0]\n",
    "    in_ch = conv.weight.shape[1]\n",
    "    print(f\"  {i}    | {out_ch:12} | {in_ch:2}‚Üí{out_ch:2}  | Downsample + channel expansion\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Same structure as acoustic encoder: {len(semantic_encoder.downsample_layers)} downsample stages\")\n",
    "print(f\"   Channel progression: 1 ‚Üí 32 ‚Üí 64 ‚Üí 128 ‚Üí 256 ‚Üí 512 ‚Üí 1024 ‚Üí 2048\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Semantic Encoder Head Layer (Final Layer) ===\n",
      "Layer: SConv1d(\n",
      "  (conv): NormConv1d(\n",
      "    (conv): Conv1d(2048, 128, kernel_size=(7,), stride=(1,))\n",
      "    (norm): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Conv1d:\n",
      "  Weight shape: torch.Size([128, 2048, 7]), dtype: torch.bfloat16\n",
      "  Dimensions: [0]=128 (out_channels=128, semantic_latent_dim), [1]=2048 (in_channels), [2]=7 (kernel)\n",
      "  Purpose: Final projection to 128D semantic latent space\n",
      "  Input: [batch, 2048, time] ‚Üí Output: [batch, 128, time]\n",
      "\n",
      "‚úÖ Outputs 128D semantic latents (vs 64D for acoustic)\n"
     ]
    }
   ],
   "source": [
    "# 6b. Semantic Tokenizer Encoder - Head Layer (Final projection to 128D)\n",
    "semantic_head = semantic_encoder.head.conv.conv\n",
    "print(f\"=== Semantic Encoder Head Layer (Final Layer) ===\")\n",
    "print(f\"Layer: {semantic_encoder.head}\")\n",
    "print(f\"\\nConv1d:\")\n",
    "print(f\"  Weight shape: {semantic_head.weight.shape}, dtype: {semantic_head.weight.dtype}\")\n",
    "print(f\"  Dimensions: [0]={semantic_head.weight.shape[0]} (out_channels=128, semantic_latent_dim), [1]={semantic_head.weight.shape[1]} (in_channels), [2]={semantic_head.weight.shape[2]} (kernel)\")\n",
    "print(f\"  Purpose: Final projection to 128D semantic latent space\")\n",
    "print(f\"  Input: [batch, {semantic_head.weight.shape[1]}, time] ‚Üí Output: [batch, 128, time]\")\n",
    "print(f\"\\n‚úÖ Outputs 128D semantic latents (vs 64D for acoustic)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SpeechConnector (Acoustic) ===\n",
      "SpeechConnector(\n",
      "  (fc1): Linear(in_features=64, out_features=1536, bias=True)\n",
      "  (norm): LlamaRMSNorm((1536,), eps=1e-06)\n",
      "  (fc2): Linear(in_features=1536, out_features=1536, bias=True)\n",
      ")\n",
      "\n",
      "Purpose: Maps 64D acoustic latents ‚Üí 1536D language model space\n",
      "\n",
      "--- Layer 1: fc1 ---\n",
      "  Weight shape: torch.Size([1536, 64]), dtype: torch.bfloat16\n",
      "  Bias shape: torch.Size([1536])\n",
      "  Dimensions: [0]=1536 (output=1536, hidden_size), [1]=64 (input=64, acoustic_latent_dim)\n",
      "  Purpose: Projects 64D acoustic latents to 1536D\n",
      "\n",
      "--- Layer 2: norm (RMSNorm) ---\n",
      "  Weight shape: torch.Size([1536]), dtype: torch.bfloat16\n",
      "  Dimensions: [0]=1536 (hidden_size=1536)\n",
      "  Purpose: Normalizes after first projection\n",
      "\n",
      "--- Layer 3: fc2 ---\n",
      "  Weight shape: torch.Size([1536, 1536]), dtype: torch.bfloat16\n",
      "  Bias shape: torch.Size([1536])\n",
      "  Dimensions: [0]=1536 (output=1536, hidden_size), [1]=1536 (input=1536, hidden_size)\n",
      "  Purpose: Second projection (1536‚Üí1536) for refinement\n",
      "\n",
      "‚úÖ Flow: 64D ‚Üí fc1 ‚Üí 1536D ‚Üí norm ‚Üí fc2 ‚Üí 1536D (ready for LM)\n"
     ]
    }
   ],
   "source": [
    "# 7. Acoustic Connector - Complete Structure\n",
    "connector = model.model.acoustic_connector\n",
    "print(\"=== SpeechConnector (Acoustic) ===\")\n",
    "print(connector)\n",
    "print(f\"\\nPurpose: Maps 64D acoustic latents ‚Üí 1536D language model space\")\n",
    "print(f\"\\n--- Layer 1: fc1 ---\")\n",
    "fc1 = connector.fc1\n",
    "print(f\"  Weight shape: {fc1.weight.shape}, dtype: {fc1.weight.dtype}\")\n",
    "print(f\"  Bias shape: {fc1.bias.shape if fc1.bias is not None else None}\")\n",
    "print(f\"  Dimensions: [0]={fc1.weight.shape[0]} (output=1536, hidden_size), [1]={fc1.weight.shape[1]} (input=64, acoustic_latent_dim)\")\n",
    "print(f\"  Purpose: Projects 64D acoustic latents to 1536D\")\n",
    "\n",
    "print(f\"\\n--- Layer 2: norm (RMSNorm) ---\")\n",
    "norm = connector.norm\n",
    "print(f\"  Weight shape: {norm.weight.shape}, dtype: {norm.weight.dtype}\")\n",
    "print(f\"  Dimensions: [0]={norm.weight.shape[0]} (hidden_size=1536)\")\n",
    "print(f\"  Purpose: Normalizes after first projection\")\n",
    "\n",
    "print(f\"\\n--- Layer 3: fc2 ---\")\n",
    "fc2 = connector.fc2\n",
    "print(f\"  Weight shape: {fc2.weight.shape}, dtype: {fc2.weight.dtype}\")\n",
    "print(f\"  Bias shape: {fc2.bias.shape if fc2.bias is not None else None}\")\n",
    "print(f\"  Dimensions: [0]={fc2.weight.shape[0]} (output=1536, hidden_size), [1]={fc2.weight.shape[1]} (input=1536, hidden_size)\")\n",
    "print(f\"  Purpose: Second projection (1536‚Üí1536) for refinement\")\n",
    "\n",
    "print(f\"\\n‚úÖ Flow: 64D ‚Üí fc1 ‚Üí 1536D ‚Üí norm ‚Üí fc2 ‚Üí 1536D (ready for LM)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SpeechConnector (Semantic) ===\n",
      "SpeechConnector(\n",
      "  (fc1): Linear(in_features=128, out_features=1536, bias=True)\n",
      "  (norm): LlamaRMSNorm((1536,), eps=1e-06)\n",
      "  (fc2): Linear(in_features=1536, out_features=1536, bias=True)\n",
      ")\n",
      "\n",
      "Purpose: Maps 128D semantic latents ‚Üí 1536D language model space\n",
      "\n",
      "--- Layer 1: fc1 ---\n",
      "  Weight shape: torch.Size([1536, 128]), dtype: torch.bfloat16\n",
      "  Bias shape: torch.Size([1536])\n",
      "  Dimensions: [0]=1536 (output=1536, hidden_size), [1]=128 (input=128, semantic_latent_dim)\n",
      "  Purpose: Projects 128D semantic latents to 1536D\n",
      "\n",
      "--- Layer 2: norm (RMSNorm) ---\n",
      "  Weight shape: torch.Size([1536]), dtype: torch.bfloat16\n",
      "  Dimensions: [0]=1536 (hidden_size=1536)\n",
      "  Purpose: Normalizes after first projection\n",
      "\n",
      "--- Layer 3: fc2 ---\n",
      "  Weight shape: torch.Size([1536, 1536]), dtype: torch.bfloat16\n",
      "  Bias shape: torch.Size([1536])\n",
      "  Dimensions: [0]=1536 (output=1536, hidden_size), [1]=1536 (input=1536, hidden_size)\n",
      "  Purpose: Second projection (1536‚Üí1536) for refinement\n",
      "\n",
      "‚úÖ Flow: 128D ‚Üí fc1 ‚Üí 1536D ‚Üí norm ‚Üí fc2 ‚Üí 1536D (ready for LM)\n",
      "\n",
      "‚ö†Ô∏è  Note: Same structure as acoustic connector, but input is 128D instead of 64D\n"
     ]
    }
   ],
   "source": [
    "# 8. Semantic Connector - Complete Structure\n",
    "connector = model.model.semantic_connector\n",
    "print(\"=== SpeechConnector (Semantic) ===\")\n",
    "print(connector)\n",
    "print(f\"\\nPurpose: Maps 128D semantic latents ‚Üí 1536D language model space\")\n",
    "print(f\"\\n--- Layer 1: fc1 ---\")\n",
    "fc1 = connector.fc1\n",
    "print(f\"  Weight shape: {fc1.weight.shape}, dtype: {fc1.weight.dtype}\")\n",
    "print(f\"  Bias shape: {fc1.bias.shape if fc1.bias is not None else None}\")\n",
    "print(f\"  Dimensions: [0]={fc1.weight.shape[0]} (output=1536, hidden_size), [1]={fc1.weight.shape[1]} (input=128, semantic_latent_dim)\")\n",
    "print(f\"  Purpose: Projects 128D semantic latents to 1536D\")\n",
    "\n",
    "print(f\"\\n--- Layer 2: norm (RMSNorm) ---\")\n",
    "norm = connector.norm\n",
    "print(f\"  Weight shape: {norm.weight.shape}, dtype: {norm.weight.dtype}\")\n",
    "print(f\"  Dimensions: [0]={norm.weight.shape[0]} (hidden_size=1536)\")\n",
    "print(f\"  Purpose: Normalizes after first projection\")\n",
    "\n",
    "print(f\"\\n--- Layer 3: fc2 ---\")\n",
    "fc2 = connector.fc2\n",
    "print(f\"  Weight shape: {fc2.weight.shape}, dtype: {fc2.weight.dtype}\")\n",
    "print(f\"  Bias shape: {fc2.bias.shape if fc2.bias is not None else None}\")\n",
    "print(f\"  Dimensions: [0]={fc2.weight.shape[0]} (output=1536, hidden_size), [1]={fc2.weight.shape[1]} (input=1536, hidden_size)\")\n",
    "print(f\"  Purpose: Second projection (1536‚Üí1536) for refinement\")\n",
    "\n",
    "print(f\"\\n‚úÖ Flow: 128D ‚Üí fc1 ‚Üí 1536D ‚Üí norm ‚Üí fc2 ‚Üí 1536D (ready for LM)\")\n",
    "print(f\"\\n‚ö†Ô∏è  Note: Same structure as acoustic connector, but input is 128D instead of 64D\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Prediction Head (Diffusion Head)\n",
    "\n",
    "Generates acoustic latents using diffusion, conditioned on language model hidden states. Has **4 HeadLayer blocks** plus a final layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VibeVoiceDiffusionHead ===\n",
      "Number of HeadLayer blocks: 4\n",
      "\n",
      "--- Input Projection 1: noisy_images_proj ---\n",
      "  Weight shape: torch.Size([1536, 64]), dtype: torch.bfloat16\n",
      "  Dimensions: [0]=1536 (output=1536, hidden_size), [1]=64 (input=64, acoustic_latent_dim)\n",
      "  Purpose: Projects noisy acoustic latents to hidden_size for diffusion\n",
      "\n",
      "--- Input Projection 2: cond_proj ---\n",
      "  Weight shape: torch.Size([1536, 1536]), dtype: torch.bfloat16\n",
      "  Dimensions: [0]=1536 (output=1536, cond_dim), [1]=1536 (input=1536, hidden_size)\n",
      "  Purpose: Projects condition (LM hidden state) to cond_dim\n",
      "\n",
      "--- Timestep Embedder ---\n",
      "  TimestepEmbedder(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1536, bias=False)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "  )\n",
      ")\n",
      "  Purpose: Embeds diffusion timestep for conditioning\n",
      "  Structure: MLP that processes timestep ‚Üí cond_dim\n"
     ]
    }
   ],
   "source": [
    "# 9. Prediction Head - Input Projections\n",
    "head = model.model.prediction_head\n",
    "print(\"=== VibeVoiceDiffusionHead ===\")\n",
    "print(f\"Number of HeadLayer blocks: {len(head.layers)}\")\n",
    "print(f\"\\n--- Input Projection 1: noisy_images_proj ---\")\n",
    "noisy_proj = head.noisy_images_proj\n",
    "print(f\"  Weight shape: {noisy_proj.weight.shape}, dtype: {noisy_proj.weight.dtype}\")\n",
    "print(f\"  Dimensions: [0]={noisy_proj.weight.shape[0]} (output=1536, hidden_size), [1]={noisy_proj.weight.shape[1]} (input=64, acoustic_latent_dim)\")\n",
    "print(f\"  Purpose: Projects noisy acoustic latents to hidden_size for diffusion\")\n",
    "\n",
    "print(f\"\\n--- Input Projection 2: cond_proj ---\")\n",
    "cond_proj = head.cond_proj\n",
    "print(f\"  Weight shape: {cond_proj.weight.shape}, dtype: {cond_proj.weight.dtype}\")\n",
    "print(f\"  Dimensions: [0]={cond_proj.weight.shape[0]} (output=1536, cond_dim), [1]={cond_proj.weight.shape[1]} (input=1536, hidden_size)\")\n",
    "print(f\"  Purpose: Projects condition (LM hidden state) to cond_dim\")\n",
    "\n",
    "print(f\"\\n--- Timestep Embedder ---\")\n",
    "t_embedder = head.t_embedder\n",
    "print(f\"  {t_embedder}\")\n",
    "print(f\"  Purpose: Embeds diffusion timestep for conditioning\")\n",
    "print(f\"  Structure: MLP that processes timestep ‚Üí cond_dim\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HeadLayer 0 (of 4) ===\n",
      "\n",
      "‚ö†Ô∏è  This layer pattern repeats for all 4 HeadLayer blocks\n",
      "\n",
      "--- FFN (Feed-Forward Network) ---\n",
      "\n",
      "gate_proj:\n",
      "  ffn.gate_proj.weight.shape = torch.Size([4608, 1536])\n",
      "  ffn.gate_proj.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [out=4608, in=1536]\n",
      "\n",
      "up_proj:\n",
      "  ffn.up_proj.weight.shape = torch.Size([4608, 1536])\n",
      "  ffn.up_proj.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [out=4608, in=1536]\n",
      "\n",
      "down_proj:\n",
      "  ffn.down_proj.weight.shape = torch.Size([1536, 4608])\n",
      "  ffn.down_proj.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [out=1536, in=4608]\n",
      "\n",
      "--- Norm ---\n",
      "  norm.weight.shape = torch.Size([1536])\n",
      "  norm.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [1536] (1D tensor)\n",
      "\n",
      "--- Adaptive Layer Norm Modulation (adaLN) ---\n",
      "  Sequential(\n",
      "  (0): SiLU()\n",
      "  (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
      ")\n",
      "  adaLN[1].weight.shape = torch.Size([4608, 1536])\n",
      "  adaLN[1].weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [out=4608, in=1536]\n",
      "  Purpose: Modulates layer norm based on condition (timestep + text context)\n",
      "\n",
      "‚ö†Ô∏è  This HeadLayer structure repeats in all 4 layers\n"
     ]
    }
   ],
   "source": [
    "# 10. Prediction Head - HeadLayer 0 (Processing Blocks)\n",
    "prediction_head = model.model.prediction_head  # Use full path to avoid variable overwrite\n",
    "head_layer_0 = prediction_head.layers[0]\n",
    "print(f\"=== HeadLayer 0 (of {len(prediction_head.layers)}) ===\")\n",
    "print(f\"\\n‚ö†Ô∏è  This layer pattern repeats for all {len(prediction_head.layers)} HeadLayer blocks\")\n",
    "print(f\"\\n--- FFN (Feed-Forward Network) ---\")\n",
    "ffn = head_layer_0.ffn\n",
    "print(f\"\\ngate_proj:\")\n",
    "print(f\"  ffn.gate_proj.weight.shape = {ffn.gate_proj.weight.shape}\")\n",
    "print(f\"  ffn.gate_proj.weight.dtype = {ffn.gate_proj.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [out={ffn.gate_proj.weight.shape[0]}, in={ffn.gate_proj.weight.shape[1]}]\")\n",
    "print(f\"\\nup_proj:\")\n",
    "print(f\"  ffn.up_proj.weight.shape = {ffn.up_proj.weight.shape}\")\n",
    "print(f\"  ffn.up_proj.weight.dtype = {ffn.up_proj.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [out={ffn.up_proj.weight.shape[0]}, in={ffn.up_proj.weight.shape[1]}]\")\n",
    "print(f\"\\ndown_proj:\")\n",
    "print(f\"  ffn.down_proj.weight.shape = {ffn.down_proj.weight.shape}\")\n",
    "print(f\"  ffn.down_proj.weight.dtype = {ffn.down_proj.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [out={ffn.down_proj.weight.shape[0]}, in={ffn.down_proj.weight.shape[1]}]\")\n",
    "\n",
    "print(f\"\\n--- Norm ---\")\n",
    "norm = head_layer_0.norm\n",
    "print(f\"  norm.weight.shape = {norm.weight.shape}\")\n",
    "print(f\"  norm.weight.dtype = {norm.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [{norm.weight.shape[0]}] (1D tensor)\")\n",
    "\n",
    "print(f\"\\n--- Adaptive Layer Norm Modulation (adaLN) ---\")\n",
    "adaLN = head_layer_0.adaLN_modulation\n",
    "print(f\"  {adaLN}\")\n",
    "adaLN_linear = adaLN[1]\n",
    "print(f\"  adaLN[1].weight.shape = {adaLN_linear.weight.shape}\")\n",
    "print(f\"  adaLN[1].weight.dtype = {adaLN_linear.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [out={adaLN_linear.weight.shape[0]}, in={adaLN_linear.weight.shape[1]}]\")\n",
    "print(f\"  Purpose: Modulates layer norm based on condition (timestep + text context)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  This HeadLayer structure repeats in all {len(prediction_head.layers)} layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FinalLayer ===\n",
      "\n",
      "--- Final Norm ---\n",
      "  norm_final.weight = None\n",
      "  Purpose: Final normalization (no learnable params)\n",
      "\n",
      "--- Output Projection (linear) ---\n",
      "  linear.weight.shape = torch.Size([64, 1536])\n",
      "  linear.weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [out=64, in=1536]\n",
      "  Purpose: Projects [B, 1536] ‚Üí [B, 64]\n",
      "  Note: '64D' output means shape [B, 64] where B=batch size\n",
      "\n",
      "--- Final adaLN Modulation ---\n",
      "  adaLN_final[1].weight.shape = torch.Size([3072, 1536])\n",
      "  adaLN_final[1].weight.dtype = torch.bfloat16\n",
      "  Tensor shape: [out=3072, in=1536]\n",
      "  Purpose: Final adaptive modulation before output\n",
      "\n",
      "‚úÖ This is the final layer - outputs 64D acoustic latents for diffusion\n",
      "   Flow: [B, 1536] ‚Üí linear ‚Üí [B, 64]\n",
      "   Where '64D' means tensor shape [B, 64]\n"
     ]
    }
   ],
   "source": [
    "# 11. Prediction Head - Final Layer (Output Projection)\n",
    "prediction_head = model.model.prediction_head  # Use full path to avoid variable overwrite\n",
    "final_layer = prediction_head.final_layer\n",
    "print(\"=== FinalLayer ===\")\n",
    "print(f\"\\n--- Final Norm ---\")\n",
    "norm_final = final_layer.norm_final\n",
    "print(f\"  norm_final.weight = {norm_final.weight if norm_final.weight is not None else 'None'}\")\n",
    "print(f\"  Purpose: Final normalization (no learnable params)\")\n",
    "\n",
    "print(f\"\\n--- Output Projection (linear) ---\")\n",
    "linear = final_layer.linear\n",
    "print(f\"  linear.weight.shape = {linear.weight.shape}\")\n",
    "print(f\"  linear.weight.dtype = {linear.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [out={linear.weight.shape[0]}, in={linear.weight.shape[1]}]\")\n",
    "print(f\"  Purpose: Projects [B, {linear.weight.shape[1]}] ‚Üí [B, {linear.weight.shape[0]}]\")\n",
    "print(f\"  Note: '64D' output means shape [B, 64] where B=batch size\")\n",
    "\n",
    "print(f\"\\n--- Final adaLN Modulation ---\")\n",
    "adaLN_final = final_layer.adaLN_modulation\n",
    "adaLN_final_linear = adaLN_final[1]\n",
    "print(f\"  adaLN_final[1].weight.shape = {adaLN_final_linear.weight.shape}\")\n",
    "print(f\"  adaLN_final[1].weight.dtype = {adaLN_final_linear.weight.dtype}\")\n",
    "print(f\"  Tensor shape: [out={adaLN_final_linear.weight.shape[0]}, in={adaLN_final_linear.weight.shape[1]}]\")\n",
    "print(f\"  Purpose: Final adaptive modulation before output\")\n",
    "\n",
    "print(f\"\\n‚úÖ This is the final layer - outputs 64D acoustic latents for diffusion\")\n",
    "print(f\"   Flow: [B, 1536] ‚Üí linear ‚Üí [B, 64]\")\n",
    "print(f\"   Where '64D' means tensor shape [B, 64]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
